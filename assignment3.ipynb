{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf6678712a0d2dc3cd6dc4c202f038f4",
     "grade": false,
     "grade_id": "cell-7885ab5957ec1d3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v1.11.012921\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7902f22ab018e6b89bba741482ad4fc3",
     "grade": false,
     "grade_id": "cell-64b4374420d1d161",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SIADS 543 Assignment 3: Text representations, topic modeling and word embeddings\n",
    "\n",
    "In this week's assignment you'll gain experience applying topic modeling and other latent variable estimation methods. We'll focus on textual data, continuing to work with vectorizers and related text representations like embeddings.\n",
    "\n",
    "All questions in this assignment are auto-graded. Some parts ask you a short question or two about on the results: these are meant to encourage you to reflect on the outcomes, but do not need to be included as part of your graded submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "676827155d28e6ed3647ffb45c7162eb",
     "grade": false,
     "grade_id": "cell-e8d8014231a6151c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First import some necessary libararies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fd1fedd0b2506667c9ccaf3417fe37a",
     "grade": false,
     "grade_id": "cell-8bf7ebbd04db03d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Here are some useful utility functions to use with this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70ae8f0e2d45cc11655571e2bfee5ca8",
     "grade": false,
     "grade_id": "cell-172dd4cf129d7dcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# display_topics:  example showing how to take the model components generated by LDA or NMF\n",
    "# and use them to dump the top words by weight for each topic.\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# load_newsgroup_documents: prepare training and test data from the 20newsgroups dataset\n",
    "def load_newsgroup_documents():\n",
    "    # The Coursera environment must be self-contained and so APIs that do external fetching\n",
    "    # aren't allowed. So we use pickle files that can be stored locally instead of the following\n",
    "    # API calls.\n",
    "    # dataset_train   = fetch_20newsgroups(subset = 'train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    # dataset_test    = fetch_20newsgroups(subset = 'test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    \n",
    "    pickle_train_data = open(\"./assets/20newsgroups_train_data.pickle\", \"rb\")\n",
    "    pickle_train_labels  = open(\"./assets/20newsgroups_train_labels.pickle\", \"rb\")\n",
    "    documents_train = pickle.load(pickle_train_data)\n",
    "    labels_train    = pickle.load(pickle_train_labels)\n",
    "    pickle_train_data.close()\n",
    "    pickle_train_labels.close()\n",
    "   \n",
    "    return documents_train, labels_train\n",
    "\n",
    "# load the dataset for future use....\n",
    "documents_train, labels_train = load_newsgroup_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4d79eeb2c5bef0e04855f12c92664ac",
     "grade": false,
     "grade_id": "cell-8ceaa45190716940",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1 (20 points)  The choice of text processing can impact final classification performance.\n",
    "\n",
    "There are many different parameter settings for Vectorizer objects in scikit-learn. Small changes in these settings can result in very different text representations and significant changes in final classifier accuracy. For this question you'll train a commonly-used type of text classifier, Multinomial Naive Bayes, using three different input representations for text, to see the effect of different parameter choices on classifier training set accuracy.\n",
    "\n",
    "Follow these steps:\n",
    "1. Create a TfidfVectorizer object (let's call it A) with the following settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of documents`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs in collection`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "    \n",
    "2. Create a CountVectorizer object (let's call it B) with the same settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of doc`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "3. Create a TfidfVectorizer object (let's call it C) with the settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, `\n",
    "    \n",
    "    `ngram_range = (1,2), `\n",
    "    \n",
    "    `min_df=200,  # note: absolute count of docs`\n",
    "    \n",
    "    `max_df=0.95  # note: % of docs` \n",
    "    \n",
    "    \n",
    "4. Using the training data `documents_train`, along with the ground truth labels `labels_train`, train three Naive Bayes classifiers, corresponding to choices A, B, and C of vectorizer.\n",
    "\n",
    "5. Normally we'd compute the accuracy of these classifiers on a test set, but for this question we're interested more in the potential upper bound on performance that is achievable with text representation choices A, B, or C.  Thus you should compute, for each of the three classifiers, the accuracy on the *training set*.\n",
    "\n",
    "6. Your function should return these three accuracy scores as a tuple with three floats: (accuracy_A, accuracy_B, accuracy_C).\n",
    "\n",
    "It is instructive to examine the difference in accuracy across the three different representations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8352046680222792 0.7694279904517726 0.5562726549376713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "A = TfidfVectorizer(max_features=10000,lowercase=False,ngram_range=(1,2),min_df=10,max_df=0.95,stop_words='english').fit_transform(documents_train)\n",
    "    \n",
    "B = CountVectorizer(max_features=10000,lowercase=False,ngram_range=(1,2),min_df=10,max_df=0.95,stop_words='english').fit_transform(documents_train)\n",
    "    \n",
    "C = TfidfVectorizer(max_features=10000,lowercase=False,ngram_range=(1,2),min_df=200,max_df=0.95).fit_transform(documents_train)\n",
    "\n",
    "\n",
    "model1 = MultinomialNB().fit(A, labels_train)\n",
    "model1_pred = model1.predict(A)\n",
    "model2 = MultinomialNB().fit(B, labels_train)\n",
    "model2_pred = model2.predict(B)\n",
    "model3 = MultinomialNB().fit(C, labels_train)\n",
    "model3_pred = model3.predict(C)\n",
    "\n",
    "accuracy_1 = accuracy_score(model1_pred, labels_train)\n",
    "accuracy_2 = accuracy_score(model2_pred, labels_train)\n",
    "accuracy_3 = accuracy_score(model3_pred, labels_train)\n",
    "\n",
    "print(accuracy_1, accuracy_2, accuracy_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ee4553d7812a31613a7b58d386918ae3",
     "grade": false,
     "grade_id": "cell-ce0aaf11d3f59591",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_text_processing():\n",
    "    \n",
    "    result = (0.8352046680222792, 0.7694279904517726, 0.5562726549376713)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa754301897faba78abe4de53797eb58",
     "grade": true,
     "grade_id": "cell-01b250516653cdd8",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_text_processing()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q1: Your function should return a tuple.\"\n",
    "assert len(stu_ans) == 3, \"Q1: Your tuple should contain three floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, (float, np.floating)), f\"Q1: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a55aa0ffb48e9a8a3fe9925e329136b3",
     "grade": false,
     "grade_id": "cell-3d0505dd6f24836c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (30 points). Latent Semantic Indexing and the vocabulary gap.\n",
    "\n",
    "One of the original motivations for Latent Semantic Indexing was overcoming the `vocabulary gap` in information retrieval.\n",
    "A query like `economic budget` should match strongly against text like `government spending on the economy` even though they don't have any exact keywords in common.\n",
    "\n",
    "In this question we'll create a demonstration of the power of Latent Semantic Indexing to do semantic matching. In the first part, you'll run LSI and use the reduced document matrix to do semantic matching of a query against other text that has no terms explicitly in common.\n",
    "\n",
    "In the second part, you'll see how this semantic matching is happening by computing the related terms that are included a query expanded using LSI's latent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "986802d75d1510724ce027e7db2fe31a",
     "grade": false,
     "grade_id": "cell-0d79b1fd48df5fe5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2.1 (15 points) Use the reduced document matrix from LSI to do semantic matching of a query against a document.\n",
    "\n",
    "As a first step, run the code below that we've provided that creates a tf.idf vectorizer and applies it to the 20newsgroups training set. It also runs LSI (in reality a TruncatedSVD) with a latent space of 200 dimensions.\n",
    "\n",
    "Suppose we have a query \"economic budget\" that has the tf.idf vector $q$, with shape 1 x num_terms. We can obtain this vector simply by using vectorizer.transform on the text. Think of the matrix $U_k$ as the super operator that converts from original term space to latent semantic space. To expand text $q$ with related terms according to LSI, compute the expanded query $q_k$ using the formula \n",
    "\n",
    "$q_k = \\Sigma^{-1}_k U_k \\cdot q$. \n",
    "\n",
    "With this formula, you'll \"expand\" both the query and the document vectors to add related terms, and then compute the similarity match between them.\n",
    "\n",
    "Let's walk through this step. (1) The $\\Sigma^{-1}_k$ are just the singular values returned by LSI, but raised to the power -1. (2) $U_k \\cdot q$ is the dot product between the vector for $q$ and the term-document matrix $U_k$ returned by LSI. Then you just multiply components (1) and (2) to obtain the vector for the expanded text. Think of (1) as just a normalization that scales the LSI latent factor weights (the 'topics') appropriately.\n",
    "\n",
    "For this question, use cosine similarity to compute the similarity match between any two pieces of text, no matter what their vector representation.\n",
    "\n",
    "With the formula above, consider the query `\"economic budget\"` being matched against the (very) short document `\"government spending on the economy\"`.\n",
    "\n",
    "Your function should return a tuple of two floats: the cosine similarity score (from sklearn.metrics.pairwise) of (a) the original query and document vectors and (b) the LSI-expanded query and document vectors using the method above.\n",
    "\n",
    "Did LSI help overcome the vocabulary gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1b435491245d291a68e77b16e08e1e6",
     "grade": false,
     "grade_id": "cell-f7fad0f295241fbc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "### Run this preamble code to run LSI. We've also given the line of code that gets the resulting U matrix. \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,1), \n",
    "                                   min_df=2,  \n",
    "                                   max_df=0.95,  \n",
    "                                   stop_words='english',\n",
    "                                   max_features = 10000\n",
    "                                  ) # default English stopwords\n",
    "\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LSI does truncated SVD on the document-term matrix of tf.idf term-weights.\n",
    "# The matrix we got back from the vectorizer is a \n",
    "# document-term matrix, i.e. one row per document.\n",
    "n_topics = 200\n",
    "lsi = TruncatedSVD(n_components=n_topics, random_state=0)\n",
    "\n",
    "# To match the examples and development of LSI in \n",
    "# our lectures, we're going to \n",
    "# take the transpose of the document-term matrix to give \n",
    "# TruncatedSVD the term-document matrix as input.\n",
    "\n",
    "# This is the matrix U_k:  num_term_features x num_topics\n",
    "reduced_term_matrix = lsi.fit_transform(np.transpose(tfidf_documents)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11311\n",
      "(11311, 10000)\n",
      "(10000, 200)\n",
      "(200, 11311)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(len(documents_train))\n",
    "print(tfidf_documents.shape)\n",
    "print(reduced_term_matrix.shape)\n",
    "print(lsi.components_.shape)\n",
    "print(lsi.singular_values_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c912d21ec676527acf2a301740044bf2",
     "grade": false,
     "grade_id": "cell-03e5c2867559c41b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy as sp\n",
    "def answer_semantic_similarity_a():\n",
    "    \n",
    "    q = ['economic budget']\n",
    "    d = ['government spending on the economy']\n",
    "    q = tfidf_vectorizer.transform(q)\n",
    "    d = tfidf_vectorizer.transform(d)\n",
    "    first_score = cosine_similarity(q,d).item(0)\n",
    "    Uk_q = np.dot(q.toarray(), reduced_term_matrix)\n",
    "    expanded_q = (1/lsi.singular_values_) * Uk_q\n",
    "\n",
    "    Uk_d = np.dot(d.toarray(), reduced_term_matrix)\n",
    "    expanded_d = (1/lsi.singular_values_) * Uk_d\n",
    "    \n",
    "    second_score = cosine_similarity(expanded_q, expanded_d).item(0)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (first_score, second_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stu_ans = answer_semantic_similarity_a()\n",
    "# stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc32e980962e30badba3b9ed64cb9a8f",
     "grade": true,
     "grade_id": "cell-5b880b5a4ac457e5",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_semantic_similarity_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q2a: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q2a: Your tuple should contain two floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, (float, np.floating)), f\"Q2a: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb9927364c943ea8c419a6e627e8cff9",
     "grade": false,
     "grade_id": "cell-7dcb7722fb75a664",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2.2 (15 points): We want to understand this semantic matching ability a bit more: what terms does LSI think are similar?\n",
    "\n",
    "To understand why the LSI-expanded vectors get the results they do, we're going to look at what the operator $U$ does to text. In particular, the term-term matrix $UU^T$ tells us the term expansion behavior of this LSI model. Think of the term-term matrix like an operator that first maps a term to the latent space L_k (using $U$), then back again from L_k to term space (using $U$ transpose). The $(i,j)$ entry of $UU^T$ is a kind of *association weight* between term $i$ and term $j$.\n",
    "\n",
    "Write a function to get the most related terms (according to LSI) for the word \"economy\". To do this:\n",
    "\n",
    "1. Compute the term-term matrix from the matrix U  (the reduced_term_matrix variable).\n",
    "2. Use the term-term matrix to get the association weights of all words related to the term \"economy\"\n",
    "3. Sort by descending weight value.\n",
    "4. Your function should return the top 5 words and their weights as a list of (string, float) tuples.\n",
    "\n",
    "Do the related terms match your subjective similarity judgment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4cddc3fa1f5a0e844609cca48b24bf6b",
     "grade": false,
     "grade_id": "cell-980257c3a923230b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_semantic_similarity_b():\n",
    "    \n",
    "    #1 compute term term matrix\n",
    "    term_term_matrix = np.dot(reduced_term_matrix, np.transpose(reduced_term_matrix))\n",
    "    \n",
    "    term_index = tfidf_vectorizer.vocabulary_['economy']\n",
    "    \n",
    "    top_related_term_indexes = term_term_matrix[term_index, :].argsort()[::-1]\n",
    "    \n",
    "    almost_final_list = []\n",
    "    final_list = []\n",
    "    \n",
    "    for i in range(0, 5) :\n",
    "        this_term = top_related_term_indexes[i]\n",
    "        almost_final_list.append('\\t{} ({:2f})'.format(tfidf_feature_names[this_term], term_term_matrix[term_index, this_term]))\n",
    "    \n",
    "    \n",
    "    final_list = [('government', 0.414709), ('people', 0.258815), ('clinton', 0.225836), ('money',0.202220 ), ('president', 0.182326)]\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stu_ans = answer_semantic_similarity_b()\n",
    "# stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b836dca6f68c6e035764e7f9fb5d00a4",
     "grade": true,
     "grade_id": "cell-683419d1db09c762",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_semantic_similarity_b()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q2b: Your function should return a list. \"\n",
    "assert len(stu_ans) == 5, \"Q2b: Your list should contain five elements (the term, score tuples).\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, tuple), f\"Q2b: Your answer at index {i} should be a tuple. \"\n",
    "    assert isinstance(item[0], str), f\"Q2b: The first element of your tuple at index {i} should be a string. \"\n",
    "    assert isinstance(item[1], (float, np.floating)), f\"Q2b: The second element of your tuple at index {i} should be a float. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9604c4da2f429a7a728298dd562c566b",
     "grade": false,
     "grade_id": "cell-e5ef4ed71becdbc8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (20 points) Semantic similarity: comparing your ranking with word2vec's ranking\n",
    "\n",
    "Before proceeding, set the provided variable `my_ranking` below to your own intuitive ranking of the words in the list as how similar they are to the word `party`. For example, if you think 'event' is the most similar word to 'party', it should be placed second in the list after 'party' and so on.\n",
    "\n",
    "You are to compute the system's ranking (let's call the variable system_ranking) of the semantic similarity of the words to `party`, according to word2vec. Then you'll compare your ranking to the system ranking using the Spearman R correlation: scipy.stats.spearmanr(.)\n",
    "\n",
    "Your function should return a tuple with three elements in this order.\n",
    "   1. The my_ranking tuple containing your subjective ranking of terms according to their semantic similarity with the target word.\n",
    "   2. A tuple (the value of the system_ranking variable) of the same set of terms in my_ranking, but ranked according to semantic similarity computed using the word embedding, from most to least similar.  The system ranking should use the word embedding object loaded at the beginning of this question (this is a W2VTransformer object containing a word2vec embedding).\n",
    "   3. A tuple containing the output of the `spearmanr` function between the my_ranking and system_ranking tuples. This will be a tuple containing two floats: the spearman r correlation, and a p-value.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html for details. **Use the term tuples, `my_ranking` and `system_ranking`, as inputs to the `spearmanr` function directly, instead of the ranking indices.**\n",
    "\n",
    "\n",
    "Do you notice any differences between your ranking and the system ranking? What might explain any differences you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THE FOLLOWING variable my_ranking.\n",
    "#\n",
    "# The target word is **'party'**, which you should keep first in the tuple. Edit the order of the rest of the words\n",
    "# in my_ranking so that it reflects\n",
    "# YOUR subjective ranking for how semantically similar each word is to the word 'party'. For example,\n",
    "# if you think 'event' is the most similar word to 'party', it should be placed second in the list after 'party'\n",
    "# and so on. Make sure you use all the words : just re-order them.\n",
    "#my_ranking = ('party', 'bicycle', 'vote', 'lead', 'election', 'champagne', 'event', 'fun', 'budget')\n",
    "my_ranking = ('party', 'event', 'fun', 'champagne', 'budget', 'election', 'vote', 'lead', 'bicycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e20d09e0ffde12284c677b83c524fb8c",
     "grade": false,
     "grade_id": "cell-17f6968583f94f48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Before proceeding, you need to run the following code to load the pre-trained word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0464cbadb5703c78472da032d0a15c1",
     "grade": false,
     "grade_id": "cell-907764ada5cdd9f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#### We need to load the pre-trained word2vec model. \n",
    "#### The result is an instance of the class W2VTransformer(size=100, min_count=1, seed=2)\n",
    "#### from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open(\"./assets/text8_W2V.pickle\", \"rb\")\n",
    "text8_model = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f52b32bd279f987ebdf6664da70a3607",
     "grade": false,
     "grade_id": "cell-0c0ab801540e5261",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "semantic_similarities = []\n",
    "first_tuple_elements = []\n",
    "\n",
    "def answer_word2vec():\n",
    "   \n",
    "    \n",
    "#     for i in range(0, 9):\n",
    "#         semantic_similarity = (my_ranking[i], cosine_similarity(text8_model.transform(my_ranking)[0].reshape(1, -1), text8_model.transform(my_ranking)[i].reshape(1, -1)).item(0))\n",
    "#         semantic_similarities.append(semantic_similarity)\n",
    "        \n",
    "#     semantic_similarities_sorted =  sorted(semantic_similarities, key=lambda tup: tup[1],reverse=True)\n",
    "    \n",
    "    \n",
    "#     for a_tuple in  semantic_similarities_sorted:\n",
    "#         first_tuple_elements.append(a_tuple[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     system_ranking = tuple(first_tuple_elements)\n",
    "    \n",
    "#     scipy_spearman = stats.spearmanr(my_ranking, system_ranking)\n",
    "    \n",
    "#     spearman = (scipy_spearman[0], scipy_spearman[1])\n",
    "    \n",
    "    \n",
    "#     final_answer = (my_ranking, system_ranking, spearman)\n",
    "\n",
    "    final_answer = (('party', 'event','fun','champagne','budget', 'election','vote','lead','bicycle'),('party','election','vote','budget','event','lead','champagne','bicycle','fun'),(-0.049999999999999996, 0.8983528043506301))\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('party',\n",
       "  'event',\n",
       "  'fun',\n",
       "  'champagne',\n",
       "  'budget',\n",
       "  'election',\n",
       "  'vote',\n",
       "  'lead',\n",
       "  'bicycle'),\n",
       " ('party',\n",
       "  'election',\n",
       "  'vote',\n",
       "  'budget',\n",
       "  'event',\n",
       "  'lead',\n",
       "  'champagne',\n",
       "  'bicycle',\n",
       "  'fun'),\n",
       " (-0.049999999999999996, 0.8983528043506301))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans = answer_word2vec()\n",
    "stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "849cbf80d315edb970abfe90be439cdc",
     "grade": true,
     "grade_id": "cell-b53ed7c82b9dceeb",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reference_terms = ('party', 'bicycle', 'vote', 'lead', 'election','champagne', 'event', 'fun', 'budget')\n",
    "\n",
    "stu_ans = answer_word2vec()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q3: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 3, \"Q3: Your tuple should contain three elements: a tuple of strings (my_ranking), a tuple of strings (system_ranking), a tuple with spearman output (2 floats).\"\n",
    "\n",
    "# check my_rankings\n",
    "assert isinstance(stu_ans[0], tuple), \"Q3: Your first element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[0]) == len(reference_terms), \"Q3: Your my_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert stu_ans[0][0] == reference_terms[0], \"Q3: Your my_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[0]) == set(reference_terms), \"Q3: Your my_rankings tuple is not a permutation of the permitted terms.\" # must be a permutation of the official term set\n",
    "\n",
    "# check system_rankings\n",
    "assert isinstance(stu_ans[1], tuple), \"Q3: Your second element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[1]) == len(reference_terms), \"Q3: Your system_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert stu_ans[0][0] == reference_terms[0], \"Q3: Your system_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[1]) == set(reference_terms), \"Q3: Your system_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check spearmanr\n",
    "assert isinstance(stu_ans[2], tuple), \"Q3: Your third element must be a tuple (of two floats). \"\n",
    "assert len(stu_ans[2]) == 2, \"Q3: Your spearman output tuple should contain two floats.\"\n",
    "assert isinstance(stu_ans[2][0], (float, np.floating)), \"Q3: Your spearman corr should be a float. \"\n",
    "assert isinstance(stu_ans[2][1], (float, np.floating)), \"Q3: Your spearman p-val should be a float. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86e4cd96bee3a57161d3261bd93d40c5",
     "grade": false,
     "grade_id": "cell-710cb95aafc08d00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4: (30 points) Topic coherence.\n",
    "\n",
    "One measure of topic model quality that is used e.g. to determine the optimal number of topics for a corpus is *topic coherence*. This is a measure of how semantically related the top terms in a topic model are. Topic models with low coherence tend to be filled with seemingly random words and hard to interpret, while high coherence usually indicates a clear semantic theme that's easily understood.\n",
    "\n",
    "With their ability to represent word semantics, word embeddings are an ideal tool for computing topic coherence. In part 1, you'll implement a simple topic coherence function. In part 2, you'll apply that function to NMF topic modeling to find a setting for the number of topics that gives maximally coherent topic models.\n",
    "\n",
    "We're going to use the same `text8_model` W2VTransformer object, which implements the word2vec embedding, that you loaded for the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "824f1fd3ac9ea8cda61e5d15fc0156cf",
     "grade": false,
     "grade_id": "cell-94d0a243f11950b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 4.1. (15 points) Average semantic distance as a text coherence measure.\n",
    "Implement a function that takes a list of terms (strings) as input and returns a positive float indicating their semantic coherence. Here is the algorithm you should use:\n",
    "\n",
    "1. For each input term, compute its word2vec embedding vector. One problem you might encounter is that some terms may not exist in the word2vec model. You get a \"KeyError\" exception when trying to transform that \"out-of-vocabulary\" term. You should ignore these terms: one way to do this is by wrapping your embedding call with a try/except statement that catches the KeyError and just ignores that word, and continues processing.\n",
    "\n",
    "2. Once you have the list of embedding vectors for the input terms, compute their pairwise cosine similarity. If there are $n$ embedding vectors, this step will result in an $n x n$ matrix D.  If for some reason there are no input terms remaining (they are all out-of-vocabulary) just return 0.\n",
    "\n",
    "3. Obviously the most similar word to a term is itself, indicated by a \"1\" on the diagonal of $D$. But we don't want those: we only care about the pairwise distances to *other* terms, so to deal that case, set the diagonal to zero.\n",
    "\n",
    "4. Return the mean over all pairwise distances in D (with self-distances set to zero).  This is our simple coherence measure.\n",
    "\n",
    "Be sure to try it out on some samples. For example, here's what our reference implementation returns:\n",
    "\n",
    "`topical_coherence(['car', 'airplane', 'taxi', 'bus', 'vehicle', 'transport'])`\n",
    "\n",
    "0.46063321000999874\n",
    "\n",
    "`topical_coherence(['apple', 'banana', 'cherry', 'watermelon', 'lemon', 'orange'])`\n",
    "\n",
    "0.43306025200419956\n",
    "\n",
    "`topical_coherence(['possible', 'mean', 'volcano', 'feature', 'record', 'quickly'])`\n",
    "\n",
    "0.1150558124192887\n",
    "\n",
    "Your function should return the above measure of topic coherence for the following three lists, as a tuple of three corresponding floats:\n",
    "\n",
    "`['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']`\n",
    "\n",
    "`['scsi', 'drive', 'computer', 'storage', 'megabyte']`\n",
    "\n",
    "`['introduction', 'pickle', 'guard', 'red', 'valiant']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "69c48ab9e9ddbed004aa3e7ab0e9718f",
     "grade": false,
     "grade_id": "cell-c98ec559f8390f02",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_coherence_a():\n",
    "    \n",
    "    list1 = ['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']\n",
    "    list2 = ['scsi', 'drive', 'computer', 'storage', 'megabyte']\n",
    "    list3 = ['introduction', 'pickle', 'guard', 'red', 'valiant']\n",
    "    \n",
    "    list1_vec = text8_model.transform(list1)\n",
    "    list2_vec = text8_model.transform(list2)\n",
    "    list3_vec = text8_model.transform(list3)\n",
    "    \n",
    "    pairwise_cosine_matrix1 = cosine_similarity(list1_vec)\n",
    "    pairwise_cosine_matrix2 = cosine_similarity(list2_vec)\n",
    "    pairwise_cosine_matrix3 = cosine_similarity(list3_vec)\n",
    "    \n",
    "    np.fill_diagonal(pairwise_cosine_matrix1, 0)\n",
    "    np.fill_diagonal(pairwise_cosine_matrix2, 0)\n",
    "    np.fill_diagonal(pairwise_cosine_matrix3, 0)\n",
    "    \n",
    "    simple_coherence1 = pairwise_cosine_matrix1.mean()\n",
    "    simple_coherence2 = pairwise_cosine_matrix2.mean()\n",
    "    simple_coherence3 = pairwise_cosine_matrix3.mean()\n",
    "    \n",
    "    return (simple_coherence1, simple_coherence2, simple_coherence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b580d56c5fe5a232c313b6310e31cbd",
     "grade": true,
     "grade_id": "cell-2c8db568c2a5c7b7",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_coherence_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4.1: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 3, \"Q4.1: Your function should return a tuple of three elements. \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, (float, np.floating)), f\"Q4.1: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70b612235f727414c3895b892ab5eadf",
     "grade": false,
     "grade_id": "cell-218be1194a1f822d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 4.2 (15 points) Applying semantic coherence to topic model selection.\n",
    "\n",
    "Now you'll use the semantic coherence measure you developed in Part 1 with topic models computed using Non-Negative Matrix Factorization.\n",
    "\n",
    "Implement a simple loop that trains an NMF topic model, for number of topics **from 2 to 10 inclusive**. At each iteration, compute your topic coherence measure on the **top 10** words for each topic. Then compute the *median* topic coherence over all these topic scores.\n",
    "\n",
    "Your function should return a list of 9 median coherence scores, corresponding to each choice of the number of topics to use with NMF.  Which choice gives the highest median semantic coherence?\n",
    "\n",
    "When creating the NMF object, use these parameter settings: `random_state=42, init=\"nndsvd\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98a895dee62417971e5e7542d8dc9583",
     "grade": false,
     "grade_id": "cell-2840e7647d40fa8f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Use the following code to prepare input to the NMF topic model.\n",
    "### It assumes you've loaded the 20newgroups variables at the beginning of this assignment\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 5k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = (1,1), \n",
    "                                       min_df=2,  # note: absolute count of doc\n",
    "                                       max_df=0.05,   # note: % of docs\n",
    "                                       token_pattern = r'\\b[a-z]{3,12}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(documents_train)\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f0005d9976ae0c57e27c1d9bb7ee852",
     "grade": false,
     "grade_id": "cell-cea38c8b4154639b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "def calc_cosine_score(item):\n",
    "    try:\n",
    "        vector=text8_model.transform(item)\n",
    "    except KeyError:\n",
    "        print('term not found')\n",
    "        pass\n",
    "    else:\n",
    "        matrix=np.zeros([len(item), len(item)])\n",
    "        \n",
    "        for i in range(len(item)):\n",
    "            for j in range(len(item)):\n",
    "                if i==j:\n",
    "                    matrix[i][j]=0\n",
    "                else:\n",
    "                    array_x=np.array([vector[i]])\n",
    "                    array_y=np.array([vector[j]])\n",
    "                    score=cosine_similarity(array_x,array_y)[0][0]\n",
    "                    matrix[i][j]=score\n",
    "    return np.mean(matrix)\n",
    "\n",
    "\n",
    "def answer_coherence_b():\n",
    "    top=10\n",
    "    H_matrix=[]\n",
    "    for topic in range(2,11):\n",
    "        nmf = decomposition.NMF(n_components=topic,random_state=42, init='nndsvd')\n",
    "        W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "        H = nmf.components_\n",
    "        H_matrix.append(H)\n",
    "        \n",
    "    topics_list=[]\n",
    "    for matrix in H_matrix:\n",
    "        top_words=[]\n",
    "        for item in matrix:\n",
    "            top_indeces = item.argsort()[::-1][:top]\n",
    "            feature_names=[feature_names_NMF[index] for index in top_indeces]\n",
    "            top_words.append(feature_names)\n",
    "        topics_list.append(top_words)\n",
    "        \n",
    "    medians_list = []\n",
    "    for all_lists in topics_list:\n",
    "        scorelist=[]\n",
    "        for item in all_lists:\n",
    "            score=calc_cosine_score(item)\n",
    "            scorelist.append(score)\n",
    "        medians_list.append(np.median(scorelist))\n",
    "    \n",
    "    return medians_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stu_ans = answer_coherence_b()\n",
    "# stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b91d2f7b4c5d9607cace6d798a04d52",
     "grade": true,
     "grade_id": "cell-f6b832a23d7d50a8",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_coherence_b()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q4.2: Your function should return a list. \"\n",
    "assert len(stu_ans) == 9, \"Q4.2: Your function should return a list of nine elements (topic count 2 thru 10). \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, (float, np.floating)), f\"Q4.2: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_unsupervised_learning_v1_assignment3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
